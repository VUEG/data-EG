import glob
import os

DATASETS = ['agrobiodiversity_species_richness',
            'carbon_sequestration',
            'cultural_landscape_index_agro',
            'cultural_landscape_index_forest',
            'erosion_prevention',
            'flood_regulation_supply',
            'floodregulation',
            'megafauna',
            'nature_tourism',
            'pollination_flows',
            'pollination_visitprob',
            'species_richness_farmland_birds_original1',
            'species_richness_vascular_plants']

rule all:
    input:
        expand(["data/provide/{dataset}/datapackage.json"], dataset=DATASETS)

# Rule to extract the data from from the 7z archive
rule extract:
    input:
        "data/org/PG mapping for LUKE.7z"
    output:
        temp("data/org/provide")
    log:
        "log/extract.log"
    message: "Extracting .tif and .tfw files from {input}."
    shell:
        # Extra quotes needed because of the whitespace in the name. NOTE:
        # This assumes that 7zip is around.
        # NOTE: we're only extracting tif- and tfw-files. ArcGIS-specific
        # .aux.xml and .ovr files are not extracted.
        "7za e '{input}' -o{output} *.tif *.tfw -r >& {log}"

# Rule to translate all rasters using compression. Also, translation makes
# the use of TFW files unnecessary. Rename the files at the same time: get rid
# of of whitespaces and put names in full lower case.
rule translate:
    input:
        dir="data/org/provide"
    output:
        dirs=expand("data/provide/{dataset}", dataset=DATASETS),
        dst_rasters=expand("data/provide/{dataset}/{dataset}.{ext}",
                           dataset=DATASETS, ext=['tif'])
    log:
        "log/translate.log"
    message: "Translating files in {input.dir}"
    run:
        src_rasters = glob.glob("{0}/*.tif".format(input))
        # Clean the log
        shell('echo "" > {0} 2>&1'.format(log))
        for i in range(0, len(src_rasters)):
            original_raster = src_rasters[i]
            # There might be a whitespace at the end of the basename
            raster = original_raster.replace(" .", ".")
            # Replace " " with "_" and turn name into lower case
            raster = raster.replace(" ", "_").lower()
            # Place translated files to "data" dir
            output_dir = output.dirs[i]
            raster = raster.replace(input.dir, output_dir)
            shell("mkdir -p {0}".format(output_dir))
            # Traslate using gdal_translate, comress using DEFLATE
            shell('echo "Traslating {0} to {1}" >> {2} 2>&1'.format(original_raster, raster, log))
            shell("gdal_translate -of GTiff -co COMPRESS=DEFLATE '{0}' '{1}' >> {2} 2>&1".format(original_raster, raster, log))

# Rule to create datapackage metadata for the resources
rule create_metadata:
    params:
        # This Google spreadsheet holds metadata for PGs datasets.
        gspread_uri="https://docs.google.com/spreadsheets/d/1MmWfJWktF33SMscCfUzE-GhYj1X0M4HB3FOF9IbHPjk/edit?usp=sharing",
        gpsread_spreadsheet_name="EG-dmp",
        gpsread_worksheet_name="datasets",
        gpsread_credentials="scripts/secret-EGpackager-4b30d0d339ed.json"
    input:
        expand("data/provide/{dataset}/{dataset}.{ext}", dataset=DATASETS, ext=['tif'])
    output:
        expand(["data/provide/{dataset}/datapackage.json"], dataset=DATASETS)
    script:
        "scripts/create_metadata.py"
